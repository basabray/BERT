# -*- coding: utf-8 -*-
"""BERT basics, Tokenizer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dk4fvgw0kdgPNDsqnUKO0mH_HfCG2bov

## Basics of BERT
- Refer https://jalammar.github.io/illustrated-transformer/
- Bert Tokenizers transform the input text data into the format that BERT expects
"""

import tensorflow as tf
import tensorflow_hub as hub

!pip install transformers

# import necessary packages
from transformers import BertTokenizer, BertModel

# Tokenize the sequence
tokenizer = BertTokenizer.from_pretrained('bert-base-cased')  # or use 'bert-base-uncased' or choose as per the input dataset

# provide a sample input
sample1 = 'where is Himalayas in the world map?'

# Give the input that needs to be encoded
encoding = tokenizer.encode(sample1)

# display the encoded ids
print(encoding)

# display the list of strings which was encoded as ids
print(tokenizer.convert_ids_to_tokens(encoding))

sample2 = 'I will visit Disney land tomorrow with my entire class and class teachers'

bert_input = tokenizer(sample2, padding='max_length', max_length = 8, truncation=True, return_tensors="pt")

print(bert_input['input_ids'])
print(bert_input['token_type_ids'])
print(bert_input['attention_mask'])
print()

encoding2 = tokenizer.encode(sample2)
print(encoding2)
print(tokenizer.convert_ids_to_tokens(encoding2))

"""- padding : to pad each sequence to the max len we specified
- max_length : the max len of each sequence, in our case we have used 10
- truncation : if True, then the tokens in each sequence which exceeds the max len will be truncated (True in our above example)
- return_tensors : the type of tensors that will be returned. Since we are using Pytorch hence, used pt. If we use Tensorflow, then use tf

### The output var named 'bert_input' will be later used as input to the BERT model
- The first print is `'input_ids'` , which is the id representation of each token. We can decode these input ids into the actual tokens as follows:
- The second print `'token_type_ids'` helps to identify in which sequence a token belongs to. If we only have a single sequence, then all of the token type ids will be 0. For text classification, token_type_ids is optional input for BERT
- `'attention_mask'` identifies whether a token is a real word or just padding. If the token contains [CLS], [SEP], or any real word, then the mask would be 1. If the token is just padding or [PAD], then the mask would be 0
"""

example_text = tokenizer.decode(bert_input.input_ids[0])
print(example_text)

